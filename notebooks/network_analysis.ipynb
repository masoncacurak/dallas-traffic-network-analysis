{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Metrics\n",
    "\n",
    "Build the network graph, compute centrality metrics, detect communities, and save outputs to `data/processed/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa82328",
   "metadata": {},
   "source": [
    "## Optional Prep\n",
    "Run preprocessing and temporal preprocessing if processed files are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to regenerate processed data\n",
    "# !python3 ../src/preprocessing.py\n",
    "# !python3 ../src/temporal_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f85b1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import helpers / ensure the project `src` directory is on the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdc8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/masoncacurak/Downloads/CS_5483/dallas_network_analysis\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "# Resolve project root as parent of this notebook's directory\n",
    "NOTEBOOK_DIR = os.path.abspath(os.getcwd())\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, os.pardir))\n",
    "SRC_DIR = os.path.join(PROJECT_ROOT, \"src\")\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.append(SRC_DIR)\n",
    "\n",
    "from build_network import load_and_build\n",
    "from centrality_analysis import compute_centrality_measures, save_centrality_rankings\n",
    "from community_detection import detect_communities, save_communities\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08792c30",
   "metadata": {},
   "source": [
    "## Build graph for community detection\n",
    "Select a time period weight (e.g., AM, Midday, PM, Evening) or leave `period=None` to use `weight_type` (congested/freeflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63ca109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed_nodes.csv...\n",
      "Loading processed_links.csv...\n",
      "Building graph using temporal weight column 'travel_time_AM'\n",
      "Adding 21389 nodes...\n",
      "Adding 35696 edges...\n",
      "Graph build complete: 21389 nodes, 35696 edges\n",
      "Graph built for period=AM: 21389 nodes, 35696 edges\n"
     ]
    }
   ],
   "source": [
    "period_main = \"AM\"  # options: AM, Midday, PM, Evening, or None\n",
    "G = load_and_build(weight_type=\"congested\", period=period_main)\n",
    "print(f\"Graph built for period={period_main}: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325dff88",
   "metadata": {},
   "source": [
    "## Centrality metrics for each time period\n",
    "Compute centrality metrics for multiple periods and save rankings per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808fdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Centrality for AM\n",
      "Loading processed_nodes.csv...\n",
      "Loading processed_links.csv...\n",
      "Building graph using temporal weight column 'travel_time_AM'\n",
      "Adding 21389 nodes...\n",
      "Adding 35696 edges...\n",
      "Graph build complete: 21389 nodes, 35696 edges\n",
      "Computing degree centrality...\n",
      "\n",
      "Top 10 nodes by degree centrality:\n",
      " 1. Node 757: 0.000468\n",
      " 2. Node 11593: 0.000468\n",
      " 3. Node 1287: 0.000421\n",
      " 4. Node 107: 0.000374\n",
      " 5. Node 108: 0.000374\n",
      " 6. Node 126: 0.000374\n",
      " 7. Node 176: 0.000374\n",
      " 8. Node 359: 0.000374\n",
      " 9. Node 402: 0.000374\n",
      "10. Node 470: 0.000374\n",
      "Degree stats -> min: 0.000000, max: 0.000468, mean: 0.000156\n",
      "\n",
      "Computing betweenness centrality by weight...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCentrality for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m Gp = load_and_build(weight_type=\u001b[33m\"\u001b[39m\u001b[33mcongested\u001b[39m\u001b[33m\"\u001b[39m, period=p)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m centrality = \u001b[43mcompute_centrality_measures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m output_path = os.path.join(PROJECT_ROOT, \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcentrality_rankings_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m centrality_paths[p] = save_centrality_rankings(centrality, output_path=output_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/CS_5483/dallas_network_analysis/src/centrality_analysis.py:36\u001b[39m, in \u001b[36mcompute_centrality_measures\u001b[39m\u001b[34m(G)\u001b[39m\n\u001b[32m     33\u001b[39m _print_stats(\u001b[33m\"\u001b[39m\u001b[33mdegree\u001b[39m\u001b[33m\"\u001b[39m, degree_centrality)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mComputing betweenness centrality by weight...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m betweenness_centrality = \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbetweenness_centrality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m _print_top_scores(\u001b[33m\"\u001b[39m\u001b[33mbetweenness\u001b[39m\u001b[33m\"\u001b[39m, betweenness_centrality)\n\u001b[32m     38\u001b[39m _print_stats(\u001b[33m\"\u001b[39m\u001b[33mbetweenness\u001b[39m\u001b[33m\"\u001b[39m, betweenness_centrality)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 9:4\u001b[39m, in \u001b[36margmap_betweenness_centrality_5\u001b[39m\u001b[34m(G, k, normalized, weight, endpoints, seed, backend, **backend_kwargs)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/networkx/utils/backends.py:535\u001b[39m, in \u001b[36m_dispatchable._call_if_no_backends_installed\u001b[39m\u001b[34m(self, backend, *args, **kwargs)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backends:\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    531\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not implemented by \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m'\u001b[39m\u001b[33m backend. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    532\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis function is included in NetworkX as an API to dispatch to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    533\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mother backends.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    534\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/networkx/algorithms/centrality/betweenness.py:143\u001b[39m, in \u001b[36mbetweenness_centrality\u001b[39m\u001b[34m(G, k, normalized, weight, endpoints, seed)\u001b[39m\n\u001b[32m    141\u001b[39m     S, P, sigma, _ = _single_source_shortest_path_basic(G, s)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# use Dijkstra's algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     S, P, sigma, _ = \u001b[43m_single_source_dijkstra_path_basic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# accumulation\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoints:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/networkx/algorithms/centrality/betweenness.py:300\u001b[39m, in \u001b[36m_single_source_dijkstra_path_basic\u001b[39m\u001b[34m(G, s, weight)\u001b[39m\n\u001b[32m    298\u001b[39m Q = []  \u001b[38;5;66;03m# use Q as heap with (distance,node id) tuples\u001b[39;00m\n\u001b[32m    299\u001b[39m heappush(Q, (\u001b[32m0\u001b[39m, \u001b[38;5;28mnext\u001b[39m(c), s, s))\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m Q:\n\u001b[32m    301\u001b[39m     (dist, _, pred, v) = heappop(Q)\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m D:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "periods = [\"AM\", \"Midday\", \"PM\", \"Evening\"]\n",
    "centrality_paths = {}\n",
    "\n",
    "for p in periods:\n",
    "    print(f\"\\nCentrality for {p} period:\")\n",
    "    Gp = load_and_build(weight_type=\"congested\", period=p)\n",
    "    centrality = compute_centrality_measures(Gp)\n",
    "    output_path = os.path.join(PROJECT_ROOT, \"data\", \"processed\", f\"centrality_rankings_{p}.csv\")\n",
    "    centrality_paths[p] = save_centrality_rankings(centrality, output_path=output_path)\n",
    "    print(f\"Saved {p} rankings to: {output_path}\")\n",
    "\n",
    "print(\"\\nCentrality computations finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014793f",
   "metadata": {},
   "source": [
    "## Community detection\n",
    "- Run Louvain for each period-specific graph (am/midday/pm/evening), saving community assignments to `communities_<period>.csv`\n",
    "- Commented out: Run Louvain on the undirected graph built for period_main (You can set `run_gn_sample=True` to also run Girvanâ€“Newman on a sampled subgraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468b8954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Communities for AM period:\n",
      "Loading processed_nodes.csv...\n",
      "Loading processed_links.csv...\n",
      "Building graph using temporal weight column 'travel_time_AM'\n",
      "Adding 21389 nodes...\n",
      "Adding 35696 edges...\n",
      "Graph build complete: 21389 nodes, 35696 edges\n",
      "Running Louvain community detection...\n",
      "Detected 475 communities via Louvain\n",
      "Largest communities (by size): [299, 297, 290, 288, 282, 281, 276, 269, 266, 266]\n",
      "Community assignments saved to /Users/masoncacurak/Downloads/CS_5483/dallas_network_analysis/data/processed/communities_AM.csv\n",
      "Saved AM communities to: /Users/masoncacurak/Downloads/CS_5483/dallas_network_analysis/data/processed/communities_AM.csv\n",
      "Total communities: 475; top sizes: [299, 297, 290, 288, 282, 281, 276, 269, 266, 266]\n",
      "\n",
      "Communities for Midday period:\n",
      "Loading processed_nodes.csv...\n",
      "Loading processed_links.csv...\n",
      "Building graph using temporal weight column 'travel_time_Midday'\n",
      "Adding 21389 nodes...\n",
      "Adding 35696 edges...\n",
      "Graph build complete: 21389 nodes, 35696 edges\n",
      "Running Louvain community detection...\n",
      "Detected 462 communities via Louvain\n",
      "Largest communities (by size): [383, 365, 365, 352, 344, 329, 284, 276, 266, 260]\n",
      "Community assignments saved to /Users/masoncacurak/Downloads/CS_5483/dallas_network_analysis/data/processed/communities_Midday.csv\n",
      "Saved Midday communities to: /Users/masoncacurak/Downloads/CS_5483/dallas_network_analysis/data/processed/communities_Midday.csv\n",
      "Total communities: 462; top sizes: [383, 365, 365, 352, 344, 329, 284, 276, 266, 260]\n",
      "\n",
      "Communities for PM period:\n",
      "Loading processed_nodes.csv...\n",
      "Loading processed_links.csv...\n",
      "Building graph using temporal weight column 'travel_time_PM'\n",
      "Adding 21389 nodes...\n",
      "Adding 35696 edges...\n",
      "Graph build complete: 21389 nodes, 35696 edges\n",
      "Running Louvain community detection...\n",
      "Detected 517 communities via Louvain\n",
      "Largest communities (by size): [278, 274, 274, 261, 256, 256, 250, 249, 236, 223]\n",
      "Community assignments saved to /Users/masoncacurak/Downloads/CS_5483/dallas_network_analysis/data/processed/communities_PM.csv\n",
      "Saved PM communities to: /Users/masoncacurak/Downloads/CS_5483/dallas_network_analysis/data/processed/communities_PM.csv\n",
      "Total communities: 517; top sizes: [278, 274, 274, 261, 256, 256, 250, 249, 236, 223]\n",
      "\n",
      "Communities for Evening period:\n",
      "Loading processed_nodes.csv...\n",
      "Loading processed_links.csv...\n",
      "Building graph using temporal weight column 'travel_time_Evening'\n",
      "Adding 21389 nodes...\n",
      "Adding 35696 edges...\n",
      "Graph build complete: 21389 nodes, 35696 edges\n",
      "Running Louvain community detection...\n",
      "Detected 463 communities via Louvain\n",
      "Largest communities (by size): [436, 350, 347, 345, 325, 300, 296, 294, 283, 282]\n",
      "Community assignments saved to /Users/masoncacurak/Downloads/CS_5483/dallas_network_analysis/data/processed/communities_Evening.csv\n",
      "Saved Evening communities to: /Users/masoncacurak/Downloads/CS_5483/dallas_network_analysis/data/processed/communities_Evening.csv\n",
      "Total communities: 463; top sizes: [436, 350, 347, 345, 325, 300, 296, 294, 283, 282]\n",
      "\n",
      "All community computations finished\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "partition, num_comms, sizes = detect_communities(G, run_gn_sample=True)\n",
    "communities_path = save_communities(partition)\n",
    "\n",
    "print(f\"Communities saved to: {communities_path}\")\n",
    "print(f\"Total communities: {num_comms}\")\n",
    "print(f\"Top community sizes: {sizes[:10]}\")\n",
    "'''\n",
    "periods = [\"AM\", \"Midday\", \"PM\", \"Evening\"]\n",
    "community_paths = {}\n",
    "\n",
    "for p in periods:\n",
    "    print(f\"\\nCommunities for {p} period:\")\n",
    "    Gp = load_and_build(weight_type=\"congested\", period=p)\n",
    "    partition, num_comms, sizes = detect_communities(Gp, run_gn_sample=False)\n",
    "    out_path = os.path.join(PROJECT_ROOT, \"data\", \"processed\", f\"communities_{p}.csv\")\n",
    "    community_paths[p] = save_communities(partition, output_path=out_path)\n",
    "    print(f\"Saved {p} communities to: {out_path}\")\n",
    "    print(f\"Total communities: {num_comms}; top sizes: {sizes[:10]}\")\n",
    "\n",
    "print(\"\\nAll community computations finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
